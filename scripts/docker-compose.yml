version: '3.9'

services:
  backend:
    build:
      context: ..
      dockerfile: scripts/Dockerfile
    container_name: capstone_backend
    ports:
      - "8001:8001"
    volumes:
      - ..:/app
      - /app/venv
    env_file:
      - .env
    #command:uvicorn src.backend.main:app --host 0.0.0.0 --port 8000
    #Mithish please check this makes sense- removed the uvcorn command since we are not using a app style anymore
    environment:
      - LLAMA_STACK_URL=http://llama-stack:8000  # Can talk to llama-stack internally
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      llama-stack:
        condition: service_healthy
      ollama:
        condition: service_healthy
    networks:
      - capstone-network
    # Remove old uvicorn command since you're using CLI now
    stdin_open: true  # Keep container running for CLI
    tty: true

  # now llama stack container
  llama-stack:
    image: llamastack/distribution-ollama:latest
    container_name: llama-stack-server
    ports:
      - "8000:8000"  
    environment:
      - OLLAMA_HOST=http://ollama:11434
      # CHANGE: Set to "cuda" if using GPU
      - DEVICE=cpu
    volumes:
      - ./llama-stack-config.yaml:/app/llama-stack-config.yaml
      # CHANGE: Add volume for embedding model cache
      - embedding-cache:/root/.cache/huggingface
    command: >
      llama-stack-run
      --config /app/llama-stack-config.yaml
      --port 8000
      --host 0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    networks:
      - capstone-network

ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    ports:
      - "11434:11434"
    volumes:
      # CHANGE: Update path ito model
      - ollama-models:/root/.ollama
    #  Uncomment below if you have NVIDIA GPU
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    restart: unless-stopped
    networks:
      - capstone-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 5s
      retries: 5  

#since 2 containers networks portion is needed
networks:
  capstone-network:
    driver: bridge

volumes:
  ollama-models:
  embedding-cache:

