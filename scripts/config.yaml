# Llama Stack Configuration
llama_stack:
  url: "http://localhost:8000"  # Internal Docker network URL
  timeout: 120  # Request timeout in seconds

# Embedding Configuration
embedding:
  provider: "ollama"
  model: "nomic-embed-text:latest"
  dimension:  768
  batch_size: 16

# Chat/Completion Configuration 
chat:
  provider: "ollama"  
  model: "mistral:latest"
  max_tokens: 2048
  temperature: 0.7
  

# Database Configuration (for later when you connect)
database:
  url: ${VECTOR_DB_URL}
  vector_dimension: 768